{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba8154e",
   "metadata": {},
   "source": [
    "Insert your desired values in the sections marked with '###' to use the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10, 60, 10) :\n",
    "    downsampling_rates = i\n",
    "\n",
    "    epochs = ###\n",
    "\n",
    "    window_size = ###\n",
    "    train_step_size = ###\n",
    "    test_step_size = 10\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.utils.data.dataset import random_split\n",
    "    from pytorchtools import EarlyStopping\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import pickle\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    seed = ###\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    class DownsampledDataset(Dataset):\n",
    "        def __init__(self, data, downsampling_rates):\n",
    "            self.data = data\n",
    "            self.downsampling_rates = downsampling_rates\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            x = self.data[index]\n",
    "            samples = []\n",
    "            labels = []\n",
    "            for rate in range(self.downsampling_rates * 2):\n",
    "                downsampled_x = self.downsample(x, rate+1)\n",
    "                padded_x = self.pad_zeros(downsampled_x, len(x[0]))\n",
    "                samples.append(padded_x)\n",
    "                label = [0] * (self.downsampling_rates * 2)\n",
    "                label[rate] = 1\n",
    "                labels.append(label)\n",
    "            samples = torch.tensor(samples, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            return samples, labels\n",
    "\n",
    "        def downsample(self, x, rate):\n",
    "            a = None\n",
    "            if rate <= self.downsampling_rates :\n",
    "                a = [x[i][::rate] for i in range(len(x))]\n",
    "            else :\n",
    "                rate = rate - self.downsampling_rates\n",
    "                a = []\n",
    "                for i in x :\n",
    "                    b = []\n",
    "                    for j in range(0, len(i), rate*2) :\n",
    "                        b += i[j:j+rate]\n",
    "                    a.append(b)\n",
    "            return a\n",
    "\n",
    "        def pad_zeros(self, x, target_length):\n",
    "            if len(x[0]) < target_length:\n",
    "                num_zeros = target_length - len(x[0])\n",
    "                x = [sample + [0] * num_zeros for sample in x]\n",
    "            return x\n",
    "\n",
    "    class NEW1DCNN(nn.Module):\n",
    "        def __init__(self, downsampling_rates, input_dim):\n",
    "            super(NEW1DCNN, self).__init__()\n",
    "            self.downsampling_rates = downsampling_rates * 2\n",
    "            self.convs = nn.Sequential(nn.Conv1d(input_dim, 16, kernel_size=3),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "                                       nn.Conv1d(16, 32, kernel_size=3),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "                                       nn.Flatten())\n",
    "            self.fc = None\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            results = []\n",
    "            for i in range(self.downsampling_rates):\n",
    "                output = self.convs(x[:,i,:,:])\n",
    "                if self.fc is None:\n",
    "                    self.fc = nn.Linear(output.size(-1), self.downsampling_rates).to(output.device)\n",
    "                output = self.fc(output)\n",
    "                output = self.softmax(output)\n",
    "                results.append(output)\n",
    "            results = torch.stack(results, dim=0)\n",
    "            results = torch.transpose(results, 0, 1).reshape(-1, results.shape[-1])\n",
    "            return results\n",
    "\n",
    "    def train_model(data, downsampling_rates, batch_size, input_dim, epochs):\n",
    "        dataset = DownsampledDataset(data, downsampling_rates)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = NEW1DCNN(downsampling_rates, input_dim=input_dim).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=###, weight_decay=###)\n",
    "\n",
    "        saving_model_path = './saving_model/'\n",
    "        early_stopping = EarlyStopping(saving_model_path=saving_model_path, patience=###, verbose=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "\n",
    "            model.train()\n",
    "            for batch_inputs, batch_labels in train_dataloader:\n",
    "                batch_inputs = batch_inputs.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_inputs)\n",
    "                labels = batch_labels.view(-1, batch_labels.shape[-1])\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(train_dataloader)\n",
    "            torch.save(model.state_dict(), '%s_epoch_%04d.pt' % (saving_model_path, epoch))\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_inputs, batch_labels in val_dataloader:\n",
    "                    batch_inputs = batch_inputs.to(device)\n",
    "                    batch_labels = batch_labels.to(device)                \n",
    "                    outputs = model(batch_inputs)\n",
    "                    labels = batch_labels.view(-1, batch_labels.shape[-1])\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                val_loss /= len(val_dataloader)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            early_stopping(val_loss, model)        \n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        model.load_state_dict(torch.load('%s_checkpoint.pt' % (saving_model_path)))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def test_model(data, downsampling_rates, model):\n",
    "        start_time = time.time()\n",
    "        dataset = DownsampledDataset(data, downsampling_rates)\n",
    "        test_dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_input = test_input.to(device)\n",
    "            test_label = test_label.to(device)\n",
    "            outputs = model(test_input)\n",
    "            labels = test_label.view(-1, test_label.shape[-1])\n",
    "\n",
    "            anomaly_score = criterion(outputs, labels)\n",
    "            scores.append(anomaly_score.cpu().detach().numpy())\n",
    "\n",
    "        scaled_scores = min_max_scale(scores)\n",
    "        auroc = roc_auc_score(test_target, scaled_scores)\n",
    "        print('auroc:', auroc)\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"test: {execution_time} sec\")\n",
    "        return scaled_scores\n",
    "\n",
    "    def min_max_scale(scores):\n",
    "        min_val = min(scores)\n",
    "        max_val = max(scores)\n",
    "        scaled_scores = [(x - min_val) / (max_val - min_val) for x in scores]\n",
    "        return scaled_scores\n",
    "\n",
    "    def create_segments_and_labels(dataframe, window_size, step_size):\n",
    "        segments = []\n",
    "        target = []\n",
    "        for start_pos in range(0, len(dataframe), step_size):\n",
    "            end_pos = start_pos + window_size\n",
    "            if end_pos > len(dataframe):\n",
    "                break\n",
    "            segment = dataframe.iloc[start_pos:end_pos].values\n",
    "            segments.append(segment[:,:-1])\n",
    "            if 1 in segment[:,-1]:\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "\n",
    "        segments = np.array(segments).transpose((0, 2, 1))\n",
    "        segments = segments.tolist()\n",
    "\n",
    "        return segments, target\n",
    "\n",
    "    with open('###', 'rb') as f:\n",
    "        traindata = pickle.load(f)\n",
    "    traindata = pd.DataFrame(traindata)\n",
    "    traindata[traindata.columns[-1]] = traindata[traindata.columns[-1]].astype(int)\n",
    "\n",
    "    with open('###', 'rb') as f:\n",
    "        testdata = pickle.load(f)\n",
    "    testdata = pd.DataFrame(testdata)\n",
    "    testdata[testdata.columns[-1]] = testdata[testdata.columns[-1]].astype(int)\n",
    "\n",
    "    print(traindata[traindata.columns[-1]].unique())\n",
    "    print(testdata[testdata.columns[-1]].unique())\n",
    "\n",
    "    train_data, train_target = create_segments_and_labels(traindata, window_size, train_step_size)\n",
    "    test_data, test_target = create_segments_and_labels(testdata, window_size, test_step_size)\n",
    "    train_data = [x for x, y in zip(train_data, train_target) if y == 0]\n",
    "    print('train data :', len(train_data))\n",
    "    print('train target :', len(train_target))\n",
    "    print('test data :', len(test_data))\n",
    "    print('test target :', len(test_target))\n",
    "\n",
    "    batch_size = ###\n",
    "    input_dim = len(train_data[0])\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = train_model(train_data, downsampling_rates, batch_size, input_dim, epochs)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    print(f\"train: {execution_time} sec\")\n",
    "\n",
    "    scaled_scores = test_model(test_data, downsampling_rates, model)\n",
    "\n",
    "    %reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
